
\documentclass[10pt,conference]{IEEEtran}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor=blue, citecolor=blue, linkcolor=blue}

\title{Quantum Fisher-Preconditioned Reinforcement Learning for Link Adaptation in a Rayleigh Fading Channel}

\author{Oluwaseyi Giwa}

\begin{document}
\maketitle

\begin{abstract}
We revisit link adaptation---joint selection of modulation order and transmit power---over block Rayleigh fading with imperfect channel state information (CSI). We propose a Fisher-preconditioned policy gradient (QPPG) that whitens updates using a Tikhonov-regularized Fisher matrix, and we evaluate it against modern baselines (PPO, TRPO), a true Natural Policy Gradient (NPG), and a hybrid Quantum Actor--Critic (QAC) baseline. Across five scenarios capturing higher dimensionality and CSI/noise uncertainty, QPPG improves final throughput (reward) and sample efficiency, with the largest margins under uncertainty. An ablation over the regularization strength $\xi$ explains stability/robustness trade-offs and justifies our operating point.
\end{abstract}

\section{Introduction}
Adaptive modulation and power control are indispensable in modern wireless links. Reinforcement learning (RL) has emerged as a flexible tool for data-driven link adaptation, but high stochasticity from fading and noisy CSI can slow convergence and destabilize vanilla policy gradients. \emph{Geometry-aware} optimization mitigates this effect via preconditioning. We therefore ask: \textbf{Can full Fisher preconditioning deliver consistent gains for link adaptation under uncertainty?}

\textbf{Contributions.} (i) A focused application of Fisher-preconditioned policy gradients to link adaptation, with a clean public implementation and SB3 baselines. (ii) A five-scenario evaluation that stresses dimensionality and CSI/noise uncertainty, using 15 seeds for significance. (iii) A computational complexity analysis contrasting per-update cost vs.\ sample efficiency, including wall-clock profiling. (iv) A sensitivity study on the Tikhonov parameter $\xi$ that explains stability trends and selects an operating point.

\section{Related Work}
Classical RL for link adaptation spans Q-learning and policy gradients with variance reduction. Natural gradients precondition with the Fisher information for improved convergence, inspiring TRPO/PPO. In quantum-aware RL, quantum natural gradients and geometry-informed updates have been proposed; our focus is not a quantum control task but a classical link with uncertainty, providing a practical stress test for full Fisher preconditioning.

\section{Environment and Algorithm}
\subsection{Rayleigh Fading Link as an MDP}
We model a single-user block-fading channel with $N$ antennas. The state is $s_t = (\hat{h}_t, \hat{\sigma}^2_t)$ comprising a noisy channel estimate and a noise variance estimate subject to uncertainty. The action is $a_t = (m_t, p_t)$: modulation order $m_t \in \{4,16,64\}$ and power $p_t \in [0,1]$. The reward is the successfully delivered throughput, penalized by power usage.

\subsection{QPPG Update}
Let $g = \nabla_\theta J(\theta)$ be the policy gradient and let $F(\theta)$ denote the (classical) Fisher information of the policy outputs. QPPG performs the preconditioned step
$\Delta\theta = \alpha \,(F(\theta)+\xi I)^{-1} g$,
with $\xi>0$ ensuring well-conditioning under noisy estimates. The agent uses a shared backbone for discrete (modulation) and continuous (power) heads.

\subsection{Computational Complexity}
Forming $F$ via a Monte Carlo estimate requires backpropagating $\log\pi(a|s)$ per sample; inverting $(F+\xi I)$ scales cubically in the number of parameters $d$ (i.e., $\mathcal{O}(d^3)$) in the worst case. We amortize cost by reusing $F$ over mini-batches and clipping preconditioned steps. Empirically, our PyTorch profile shows a per-update overhead of $\sim$1.6--2.3$\times$ vs.\ vanilla PG for $d\approx 30$k (see Table~\ref{tab:profile}), offset by faster convergence (fewer episodes).

\begin{table}[t]
\centering
\caption{Per-update wall-clock (ms) on a CPU reference implementation.}
\label{tab:profile}
\begin{tabular}{lcc}
\toprule
Method & PG Backprop & + Fisher \& Solve \\
\midrule
Vanilla PG & 3.1 & --- \\
QPPG (ours) & 3.1 & 2.9 \\
NPG (classical) & 3.1 & 2.8 \\
\bottomrule
\end{tabular}
\end{table}

\section{Experimental Setup}
\textbf{Scenarios.} (S1) baseline $N{=}4$, pilot SNR 10\,dB, 0\,dB noise uncertainty;
(S2) $N{=}8$; (S3) pilot SNR 5\,dB; (S4) 5\,dB noise-uncertainty; (S5) $N{=}8$ plus 5\,dB uncertainty.
\textbf{Agents.} QPPG, NPG, QAC, PPO, TRPO. SB3-compatible wrapper discretizes power for PPO/TRPO.
\textbf{Seeds.} 15. \textbf{Training.} 500--1000 episodes depending on scenario.

\section{Results and Discussion}
\subsection{Performance Comparison}
Fig.~\ref{fig:curves} shows learning curves; Fig.~\ref{fig:final} summarizes final rewards with 95\% CIs. QPPG consistently matches or exceeds the strongest baseline, with the largest margins in S3--S5.

\subsection{Robustness Under Uncertainty}
Table/figures highlight that under degraded CSI and noise uncertainty (S3--S5), QPPG's preconditioning yields higher sample efficiency and final reward.

\subsection{Ablation on $\xi$}
Fig.~\ref{fig:ablation} plots final average reward vs.\ $\xi \in \{10^{-3},10^{-2},10^{-1},0.5,1.0\}$, indicating a broad optimum around $\xi=10^{-1}$ balancing stability and step-size.

\section{Conclusion}
Fisher-preconditioned policy gradients (QPPG) provide practical gains for link adaptation under uncertainty, justifying increased per-update cost by improved sample efficiency. Future work: multi-user MIMO and joint scheduling.

\section*{Artifacts}
Code, data format, and plotting scripts accompany this paper. The QPPG framework diagram (Mermaid) is supplied.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{../figures/curves_s4_noise_uncert.pdf}
\caption{Learning curves with 95\% CIs (example scenario).}
\label{fig:curves}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{../figures/final_s4_noise_uncert.pdf}
\caption{Final reward across agents with 95\% CIs (example scenario).}
\label{fig:final}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{../figures/ablation_xi.pdf}
\caption{Ablation: sensitivity to $\xi$.}
\label{fig:ablation}
\end{figure}

\bibliographystyle{IEEEtran}
\begin{thebibliography}{00}
\bibitem{SB3} A. Raffin et al., ``Stable-Baselines3,'' 2021.
\bibitem{TRPO} J. Schulman et al., ``Trust Region Policy Optimization,'' ICML, 2015.
\bibitem{PPO} J. Schulman et al., ``Proximal Policy Optimization Algorithms,'' 2017.
\end{thebibliography}

\end{document}
